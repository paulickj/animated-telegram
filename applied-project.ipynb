{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906d4563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup\n",
    "#%pip install importlib_metadata --force-reinstall\n",
    "#%pip install keras\n",
    "#%env SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True\n",
    "#%pip install wfdb\n",
    "#%pip install tqdm\n",
    "# %pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b758968c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "import keras_tuner as kt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wfdb\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f80463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU Availability\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eff10ed",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "Description of Datasets Used:\n",
    " - CHF-RR Dataset: Congestive heart failure RR interval database. It comprised patients with heart failure. This dataset contained records of 29 patients from CHF201 to CHF229.\n",
    " - NSR-RR Dataset: Normal sinus rhythm RR interval database. This dataset had 54 normal sinus rhythm recordings ranging in age from 28 to 76. Records from NSR001 through NSR054 of 54 patients were available.\n",
    " \n",
    " \n",
    "Both datasets were provided by PhysioBank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72887a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Collection\n",
    "\n",
    "# Description of Datasets Used:\n",
    "# CHF-RR Dataset: Congestive heart failure RR interval database. It comprised patients with heart failure. \n",
    "# This dataset contained records of 29 patients from CHF201 to CHF229.\n",
    "# NSR-RR Dataset: Normal sinus rhythm RR interval database. This dataset had 54 normal sinus rhythm recordings \n",
    "# ranging in age from 28 to 76. Records from NSR001 through NSR054 of 54 patients were available.\n",
    "# Both datasets were provided by PhysioBank.\n",
    "\n",
    "# Downloading Data\n",
    "\n",
    "# Function to download a database from PhysioBank\n",
    "def download(database):\n",
    "    \"\"\"\n",
    "    Downloads the specified database from PhysioBank and saves it in the current working directory.\n",
    "\n",
    "    Parameters:\n",
    "    database (str): Name of the database to download.\n",
    "    \"\"\"\n",
    "    cwd = os.getcwd()\n",
    "    dl_dir = os.path.join(cwd, database)\n",
    "    wfdb.dl_database(database, dl_dir=dl_dir)\n",
    "    print(f\"Downloaded {database} to {dl_dir}\")\n",
    "    display(os.listdir(dl_dir))\n",
    "\n",
    "# Display available databases from PhysioBank\n",
    "dbs = wfdb.get_dbs()\n",
    "print(\"Available Databases from PhysioBank:\")\n",
    "display(dbs)\n",
    "\n",
    "# Uncomment the following lines to download specific datasets\n",
    "download('nsrdb')  # Download Normal Sinus Rhythm RR Interval Database\n",
    "download('chfdb')  # Download Congestive Heart Failure RR Interval Database\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2150bd30",
   "metadata": {},
   "source": [
    "# Converting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d08f7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Data\n",
    "\n",
    "# Function to convert a wfdb record to a DataFrame\n",
    "def wfdb_to_dataframe(record):\n",
    "    \"\"\"\n",
    "    Converts a wfdb record to a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    record (wfdb.Record): The wfdb record to convert.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the signal data.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(record.p_signal, columns=record.sig_name)\n",
    "    return df\n",
    "\n",
    "# Function to load a wfdb record and convert it to a DataFrame\n",
    "def load_record(uri):\n",
    "    \"\"\"\n",
    "    Loads a wfdb record and converts it to a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    uri (str): URI of the wfdb record to load.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the signal data.\n",
    "    \"\"\"\n",
    "    record = wfdb.rdrecord(uri)\n",
    "    return wfdb_to_dataframe(record)\n",
    "\n",
    "# Example usage of load_record to load and plot a record\n",
    "a = load_record('nsrdb/16265')\n",
    "plt.plot(a['ECG1'][:212])\n",
    "plt.show()\n",
    "\n",
    "# Example usage of load_record to load and plot a different record\n",
    "b = load_record('chfdb/chf03')\n",
    "plt.plot(b['ECG1'][:212])\n",
    "plt.show()\n",
    "\n",
    "# Example usage of load_record to load and plot a different record\n",
    "b = load_record('chfdb/chf03')\n",
    "plt.plot(b['ECG2'][:212])\n",
    "plt.show()\n",
    "\n",
    "# List of all files in the 'nsrdb' directory with the '.hea' extension\n",
    "nsrdb_list = glob.glob(\"nsrdb/*.hea\")\n",
    "print(\"Number of NSRDB files:\", len(nsrdb_list))\n",
    "\n",
    "# List of all files in the 'chfdb' directory with the '.hea' extension\n",
    "chfdb_list = glob.glob(\"chfdb/*.hea\")\n",
    "print(\"Number of CHFDB files:\", len(chfdb_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b0879d",
   "metadata": {},
   "source": [
    "# Data Creation and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c110ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Creation and Processing\n",
    "\n",
    "# Function to create a database from a list of files\n",
    "def create_db(file_list, record_class, clip_duration=60, sampling_rate=212):\n",
    "    \"\"\"\n",
    "    Create a database from a list of files.\n",
    "\n",
    "    Parameters:\n",
    "    file_list (list): List of file paths for the records.\n",
    "    record_class (int): Class label for the records.\n",
    "    clip_duration (int): Duration of each clip in seconds.\n",
    "    sampling_rate (int): Sampling rate of the records.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the database.\n",
    "    \"\"\"\n",
    "    clip_len = sampling_rate * clip_duration\n",
    "    num_channels = len(load_record(file_list[0].removesuffix('.hea')).columns)\n",
    "    total_clips = sum(len(load_record(file_path.removesuffix('.hea'))) // clip_len for file_path in file_list)\n",
    "\n",
    "    # Preallocate a NumPy array for all clips\n",
    "    all_clips = np.empty((total_clips, clip_len * num_channels + 1))  # +1 for the class label\n",
    "\n",
    "    clip_index = 0\n",
    "    for file_path in file_list:\n",
    "        record_df = load_record(file_path.removesuffix('.hea'))\n",
    "        num_clips = len(record_df) // clip_len\n",
    "\n",
    "        for i in range(num_clips):\n",
    "            clip = record_df.iloc[i * clip_len: (i + 1) * clip_len].values.flatten()\n",
    "            all_clips[clip_index, :-1] = clip  # Fill all but the last column with clip data\n",
    "            all_clips[clip_index, -1] = record_class  # Last column is the class label\n",
    "            clip_index += 1\n",
    "\n",
    "    # Convert the NumPy array directly to a DataFrame\n",
    "    column_names = [f't{i}' for i in range(clip_len * num_channels)] + ['class']\n",
    "    df_clips = pd.DataFrame(all_clips, columns=column_names)\n",
    "\n",
    "    return df_clips\n",
    "\n",
    "# Create a database from 'nsrdb' records with class label 0\n",
    "nsrdb_list = glob.glob(\"nsrdb/*.hea\")\n",
    "nsrdb_df = create_db(nsrdb_list, 0)\n",
    "print(\"Shape of NSRDB DataFrame:\", nsrdb_df.shape)\n",
    "\n",
    "# Create a database from 'chfdb' records with class label 1\n",
    "chfdb_list = glob.glob(\"chfdb/*.hea\")\n",
    "chfdb_df = create_db(chfdb_list, 1)\n",
    "print(\"Shape of CHFDB DataFrame:\", chfdb_df.shape)\n",
    "\n",
    "# Concatenate the dataframes to create the full dataset\n",
    "full_df = pd.concat([nsrdb_df, chfdb_df], axis=0, ignore_index=True, sort=False)\n",
    "print(\"Shape of Full DataFrame:\", full_df.shape)\n",
    "\n",
    "# Split the dataset into features (X) and labels (y)\n",
    "y_df = full_df.pop(\"class\")\n",
    "x_df = full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60adf27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e028d6fd",
   "metadata": {},
   "source": [
    "# Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2d765f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Splitting\n",
    "\n",
    "# Split the full dataset into training, validation, and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_df, y_df, test_size=0.30)\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_train, y_train, test_size=0.30)\n",
    "\n",
    "print(\"Training Data:\", X_train.shape)\n",
    "print(\"Validation Data:\", X_validate.shape)\n",
    "print(\"Testing Data:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b2c066",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30ff900",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import datetime\n",
    "\n",
    "input_shape = (X_train.shape[1], 1)\n",
    "\n",
    "# Create a TensorBoard callback\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=input_shape),\n",
    "    tf.keras.layers.Conv1D(32, 3, activation='relu', name='conv1d_2'),\n",
    "    tf.keras.layers.BatchNormalization(name='batch_normalization_2'),\n",
    "    tf.keras.layers.MaxPooling1D(2, name='max_pooling1d_2'),\n",
    "    tf.keras.layers.Conv1D(64, 5, activation='relu', name='conv1d_3'),\n",
    "    tf.keras.layers.BatchNormalization(name='batch_normalization_3'),\n",
    "    tf.keras.layers.MaxPooling1D(2, name='max_pooling1d_3'), \n",
    "    tf.keras.layers.Conv1D(32, 5, activation='relu', name='conv1d_4'),\n",
    "    tf.keras.layers.BatchNormalization(name='batch_normalization_4'),\n",
    "    tf.keras.layers.MaxPooling1D(2, name='max_pooling1d_4'), \n",
    "    tf.keras.layers.Flatten(name='flatten_1'),\n",
    "    tf.keras.layers.Dense(32, activation='relu', name='dense_3'),\n",
    "    tf.keras.layers.Dropout(0.5, name='dropout_2'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid', name='dense_5')\n",
    "])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c59b67f",
   "metadata": {},
   "source": [
    "# Model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483d73ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "# Set the learning rate\n",
    "adam = Adam(learning_rate=0.00005)\n",
    "\n",
    "# Compile the model with the modified optimizer\n",
    "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the training data while validating on the dedicated validation set\n",
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=5, \n",
    "    validation_data=(X_validate, y_validate),\n",
    "    callbacks=[tensorboard_callback]\n",
    ")\n",
    "\n",
    "# Model Evaluation\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a3ce17",
   "metadata": {},
   "source": [
    "# Model Evaluation & Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfa74f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "\n",
    "# Generate Predictions\n",
    "y_pred_prob = model.predict(X_test)\n",
    "\n",
    "# If y_pred_prob is 3D, reduce it to 1D\n",
    "if y_pred_prob.ndim == 3:\n",
    "    y_pred_prob = y_pred_prob[:, -1, 0]\n",
    "\n",
    "# Convert probabilities to class labels\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# Generate and Plot Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calculate additional performance metrics\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "# Plotting the metrics\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "# Plot ROC Curve\n",
    "plt.subplot(1, 3, 2)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Plot Training and Validation Metrics\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print classification report with five decimal points\n",
    "report = classification_report(y_test, y_pred, target_names=['NSR', 'CHF'], digits=5)\n",
    "print(report)\n",
    "print()\n",
    "\n",
    "# Calculate ROC AUC with five decimal points\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "print(f'ROC AUC: {roc_auc:.5f}')\n",
    "print()\n",
    "\n",
    "# Print F1 score, Precision, and Recall with five decimal points\n",
    "print(f'Precision: {precision:.5f}')\n",
    "print(f'Recall: {recall:.5f}')\n",
    "print(f'F1 Score: {f1_score:.5f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
